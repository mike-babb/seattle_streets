# mike babb
# 2024 06 28
# what streets start and stop?


# standard
import os


# external
from itertools import combinations, product
import geopandas as gpd
from matplotlib.patches import Rectangle
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import pandas as pd
import seaborn as sns
from shapely.geometry import LineString, Point
from shapely import line_merge


# custom
from geodataio.geo_operations import points2distance, calculate_initial_compass_bearing








# file path
input_file_path = 'H:/project/seattle_streets/data/' 
output_file_path = 'H:/project/seattle_streets/data/individual_streets'


file_name = 'Street_Network_Database.gpkg'


fpn = os.path.join(input_file_path, file_name)


gdf = gpd.read_file(filename = fpn)


gdf.columns


# load the node data
input_file_name = 'Street_Network_Nodes.gpkg'


fpn = os.path.join(input_file_path, input_file_name)


node_gdf = gpd.read_file(filename = fpn)


node_gdf.head()


node_gdf['coords'] = node_gdf['geometry'].map(lambda x: x.coords[0])


node_gdf.head()


# zap this into a dictionary
node_dict = {}
for my_row in node_gdf.itertuples():
    node_dict[my_row.node_id] = my_row.coords


node_dict[10272]





gdf_agg = gdf[['ord_stname', 'snd_group']].drop_duplicates().groupby(['ord_stname']).agg( snd_group_count = ('snd_group', 'size')).reset_index()


gdf_agg.head()


gdf_agg['snd_group_count'].describe()


# what are the streets with the most breaks?
gdf_agg.loc[gdf_agg['snd_group_count'] == gdf_agg['snd_group_count'].max(), :]





# number of streets with at least two segments
gdf_agg.loc[gdf_agg['snd_group_count'] > 1, :].shape


1131 / gdf_agg.shape[0]


gdf_agg.loc[gdf_agg['snd_group_count'] == 1, :].shape


1379 / gdf_agg.shape[0]


# 55 percent of streets are single segment
# 45 percent of streets are multi-segments.
# TODO: figure out how much of this is road miles.


# get start and end points of each line
#gdf['s_coord'] = gdf['geometry'].map(lambda x: x.coords[0])
#gdf['e_coord'] = gdf['geometry'].map(lambda x: x.coords[-1])





def write_gdf(gdf: gpd.GeoDataFrame, output_file_path:str, output_file_name:str):
    
    ofpn = os.path.join(output_file_path, output_file_name)

    if 'coords' in gdf.columns:
        output_gdf = gdf.drop(labels = ['coords'], axis = 1)
        output_gdf.to_file(filename = ofpn, driver = 'GPKG', index = False)
    else:
        gdf.to_file(filename = ofpn, driver = 'GPKG', index = False)

    return None


def subset_node_gdf(node_gdf:gpd.GeoDataFrame, other_node_df:pd.DataFrame):
    node_subset_gdf = pd.merge(left = node_gdf, right = other_node_df)
    return node_subset_gdf


def create_graph_count_edges(gdf:gpd.GeoDataFrame, source_node:str, end_node:str):

    # create the graph for a single segment
    g = nx.from_pandas_edgelist(df = gdf, source = 'f_intr_id', target = 't_intr_id', edge_attr=True)
    node_list = list(g.nodes)

    # node distance, total hops, and number of edges per node
    node_dist_dict = {nn:0 for nn in node_list}
    node_hop_dict = {nn:0 for nn in node_list}
    # this will get the number of connected edges
    nn_edge_count_dict = {}
    
    # the snd of each node
    node_snd_group_dict = {}    
    snd_group_list = gdf['snd_group'].unique().tolist()
    for snd_group in snd_group_list:
        # use graph travesal to accumulate geographic distance
        curr_node_df = gdf.loc[gdf['snd_group'] == snd_group, ['f_intr_id', 't_intr_id']]
        curr_node_list = set(curr_node_df['f_intr_id'].tolist()).union(curr_node_df['t_intr_id'])                
        
        curr_node_snd_group_dict = {nn:snd_group for nn in curr_node_list}
        
        # combine the two dictionaries
        node_snd_group_dict = node_snd_group_dict | curr_node_snd_group_dict        
        
        # this will get the geographic distance and number of hops
        for sn, en in combinations(curr_node_list, r = 2):
            #print(sn, en)
            sp = nx.shortest_path(G = g, source = sn, target = en, weight = 'gis_seg_length')
            curr_dist = 0
            curr_hop_dist = 0        
            for i_nn, nn in enumerate(sp[:-1]):
                curr_dist += g[nn][sp[i_nn + 1]]['gis_seg_length']
                curr_hop_dist += 1            
            node_dist_dict[sn] += curr_dist
            node_dist_dict[en] += curr_dist
    
            node_hop_dict[sn] += curr_hop_dist
            node_hop_dict[en] += curr_hop_dist                
        
    for nn in node_list:
        nn_edge_count_dict[nn] = len(list(nx.dfs_edges(G = g, source=nn, depth_limit= 1)))
        
        
    # this gets the total geographic distance and total hops
    node_dist_df = pd.DataFrame.from_dict(data = node_dist_dict, orient = 'index', columns = ['tot_dist']).reset_index(names = ['node_id'])
    node_dist_df['tot_hop_dist'] = node_dist_df['node_id'].map(node_hop_dict)
    # number of edges
    node_dist_df['n_edges'] = node_dist_df['node_id'].map(nn_edge_count_dict)
    # add the snd_group
    node_dist_df['snd_group'] = node_dist_df['node_id'].map(node_snd_group_dict)        

    node_dist_df['avg_dist_per_hop'] = node_dist_df['tot_dist'] / node_dist_df['tot_hop_dist']
    # rank it!
    # greater distance indicates nodes further away from others
    node_dist_df['tot_dist_rank'] = node_dist_df.groupby(['snd_group'])['tot_dist'].rank(method = 'dense', ascending = False) 
    node_dist_df['tot_hop_dist_rank'] = node_dist_df.groupby(['snd_group'])['tot_hop_dist'].rank(method = 'dense', ascending = False)    
    # fewer edges indicates start and stop points
    node_dist_df['n_edges_rank'] = node_dist_df.groupby(['snd_group'])['n_edges'].rank(method = 'dense', ascending = True)
    # the rank per average distance - this also indicates greater distance
    node_dist_df['avg_dist_per_hop_rank'] = node_dist_df.groupby(['snd_group'])['avg_dist_per_hop'].rank(method = 'dense', ascending = True)
        
    # the nodes with the lowest combined ranks are the "start" and "end" points of the lines
    node_dist_df['combined_rank'] = node_dist_df['tot_dist_rank'] + node_dist_df['tot_hop_dist_rank'] + node_dist_df['n_edges_rank'] + node_dist_df['avg_dist_per_hop_rank']
    
    node_dist_df['final_rank'] = node_dist_df.groupby(['snd_group'])['combined_rank'].rank(method = 'dense', ascending = True)
    node_dist_df = node_dist_df.sort_values(by = ['snd_group', 'final_rank'], ascending = True)

    # build a dataframe that counts the number of edges per node:
    n_edge_count_df = node_dist_df.loc[node_dist_df['final_rank'] <= 2, :].copy()
    #print(n_edge_count_df.shape)

    node_snd_group_id_df = pd.DataFrame.from_dict(data = node_snd_group_dict, orient = 'index', columns = ['snd_group']).reset_index(names = ['node_id'])            
        
    #print(edge_count_df.head())
    #print(edge_count_df['snd_group'].value_counts())
    # update the dataframes of interest    
    #n_edge_count_df = n_edge_count_df.loc[-n_edge_count_df['node_id'].isin(drop_node_list), :].copy()
    
    snd_group_node_dict = {}
    snd_group_id_list = n_edge_count_df['snd_group'].unique().tolist()
    for snd_group in snd_group_id_list:        
        snd_group_node_dict[snd_group] = n_edge_count_df.loc[n_edge_count_df['snd_group'] == snd_group, 'node_id'].tolist()        

    
    return g, n_edge_count_df, node_dist_df, node_snd_group_id_df, node_snd_group_dict, snd_group_node_dict


sn = '10TH AVE E'


temp_gdf = gdf.loc[gdf['ord_stname'] == sn, :].copy()        


g, n_edge_count_df, node_dist_df, node_snd_group_id_df, node_snd_group_dict, snd_group_node_dict = create_graph_count_edges(gdf = temp_gdf, 
                                                                                                                  source_node = 'f_intr_id', 
                                                                                                                  end_node = 't_intr_id')



# nope, we can use k-edge augmentation with the computed distances to figure this out
# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.connectivity.edge_augmentation.k_edge_augmentation.html#networkx.algorithms.connectivity.edge_augmentation.k_edge_augmentation


s_names = gdf_agg.loc[gdf_agg['snd_group_count'] > 1, 'ord_stname'].unique().tolist()
#s_names = ['13TH AVE W']


did_it_work = False
output_gdf_list = []
# extra explode gdf
ex_gdf_list = []
ex_ex_gdf_list = []
for sn in s_names[:None]:
    # get a street by name
    print(sn)
    # subset the gdf
    temp_gdf = gdf.loc[gdf['ord_stname'] == sn, :].copy()        
    
    # build a graph to find (dis)connected components: this is an easy way to find portions of a street that are not connected to each other
    # do I need this graph?
    
    # we can do products of products to find which streets should be compared.
    # the first product to compare is the street group ids
        
    snd_group_id_list = temp_gdf['snd_group'].unique()
    #print(snd_group_id_list)
    output_file_name = '_'.join(sn.split()) + '.gpkg'
    write_gdf(gdf = temp_gdf, output_file_path = output_file_path, output_file_name = output_file_name)
    
    # hmmm, let's compute the combinations
    # figure out which nodes in a graph only have one successor
    g, n_edge_count_df, node_dist_df, node_snd_group_id_df, node_snd_group_dict, snd_group_node_dict = create_graph_count_edges(gdf = temp_gdf, 
                                                                                                                   source_node = 'f_intr_id', 
                                                                                                                   end_node = 't_intr_id')
    # create a full graph of the street segment. 
    temp_gdf['weight'] = temp_gdf['gis_seg_length']
    fg = nx.from_pandas_edgelist(df = temp_gdf, source = 'f_intr_id', target = 't_intr_id', edge_attr=True)
    #n_edge_count_df = n_edge_count_df.loc[n_edge_count_df['tot_dist_rank'] <= 2, :]
    

    # subset of nodes - these are the furthest nodes.
    node_subset_gdf = subset_node_gdf(node_gdf = node_gdf, other_node_df = n_edge_count_df)
    output_file_name = 'subset_nodes_' + '_'.join(sn.split()) + '.gpkg'
    write_gdf(gdf = node_subset_gdf, output_file_path = output_file_path, output_file_name = output_file_name)

    # full nodes - all nodes in the line segments. 
    curr_node_df = pd.DataFrame(data = {'node_id':g.nodes()})
    node_subset_gdf = subset_node_gdf(node_gdf = node_gdf, other_node_df = curr_node_df)
    output_file_name = 'full_nodes_' + '_'.join(sn.split()) + '.gpkg'
    write_gdf(gdf = node_subset_gdf, output_file_path = output_file_path, output_file_name = output_file_name)
             
    # edge_list = []
    # for snd_group in n_edge_count_df['snd_group'].unique().tolist():
    #     nodes = n_edge_count_df.loc[n_edge_count_df['snd_group']==snd_group, 'node_id'].tolist()
    #     weight = points2distance(node_dict[nodes[0]], node_dict[nodes[1]], unit = 'miles')
    #     nodes.append(weight)
    #     edge_list.append(nodes)
    
    # create a list of available metrics
    avail_edges = []
    node_dist_dict = {}
    for ne in nx.non_edges(fg):    
        if node_snd_group_dict[ne[0]] != node_snd_group_dict[ne[1]]:
            # convert to feet to match
            weight = points2distance(node_dict[ne[0]], node_dict[ne[1]], unit = 'miles') * 5280
            output = (ne[0], ne[1], {'weight':weight})
            node_dist_dict[(ne[0], ne[1])] = weight 
            node_dist_dict[(ne[1], ne[0])] = weight 
            avail_edges.append(output)

    # this is missing segments.
    data_list = []
    line_list = []
    augmented_edges = nx.k_edge_augmentation(G = fg, k = 1, avail = avail_edges, weight = 'weight')    
    for i_ne, ne in enumerate(augmented_edges):
        weight = node_dist_dict[(ne[0], ne[1])] 
        temp_data_list = [sn, i_ne, ne[0], ne[1], weight]
        temp_line = LineString([node_dict[ne[0]], node_dict[ne[1]]])
        line_list.append(temp_line)
        data_list.append(temp_data_list)

    my_output_gdf = gpd.GeoDataFrame(data = data_list,
                                     columns = ['ord_stname', 'snd_group', 'sn_id', 'en_id', 'dist'],
                                     geometry = line_list, crs = 'epsg:4326')
    my_output_gdf['same_snd_group'] = int(0)

    col_names = ['ord_stname', 'snd_group', 'f_intr_id', 't_intr_id', 'gis_seg_length', 'geometry']    
    output_gdf = temp_gdf[col_names].copy()
    output_gdf['same_snd_group'] = int(1)
    output_gdf = output_gdf.rename(columns = {'f_intr_id':'sn_id', 't_intr_id':'en_id', 'gis_seg_length':'dist'})
    
    my_output_gdf = pd.concat([output_gdf, my_output_gdf])
    #print(my_output_gdf.head())
    
    
    #my_output_gdf.loc[my_output_gdf['sn_snd_group_id'] == my_output_gdf['en_snd_group_id'], 'same_snd_group'] = 1 
    #print(my_output_gdf.head())
    
    
    output_file_name = 'missing_segments_' + '_'.join(sn.split()) + '.gpkg'
    ofpn = os.path.join(output_file_path, output_file_name)    
    
    my_output_gdf.to_file(filename = ofpn, driver = 'GPKG', index = False)

    output_gdf_list.append(my_output_gdf)
    did_it_work = True


ms_gdf = pd.concat(objs = output_gdf_list)


ms_gdf['dist_miles'] = ms_gdf['dist'] / 5280


output_file_path = 'H:/project/seattle_streets/data'
output_file_name = 'missing_segments.gpkg'
ofpn = os.path.join(output_file_path, output_file_name)    

ms_gdf.to_file(filename = ofpn, driver = 'GPKG', index = False)


did_it_work


# find the longest
ms_gdf['same_snd_group'].value_counts()


ms_gdf.shape


gdf['ord_stname'].unique().shape


wms_gdf = ms_gdf.loc[ms_gdf['same_snd_group'] == 0, :].copy()


(wms_gdf['dist']).describe()


wms_gdf['dist_rank'] = wms_gdf['dist'].rank(method = 'dense', ascending = False)


wms_gdf.loc[wms_gdf['dist_rank'] <= 10, 'ord_stname'].tolist()


temp_agg = ms_gdf[['ord_stname', 'same_snd_group']].drop_duplicates()


temp_agg.shape


wms_gdf.head()


wms_gdf['dist_miles_log'] = np.log10(wms_gdf['dist_miles'])


wms_gdf['dist_miles_log'].describe()


# let's make a graphic showing the distances
sns.set_theme(style = "whitegrid")
f, ax = plt.subplots(figsize = (20, 5))

my_plot = sns.histplot(data = wms_gdf, x = 'dist_miles_log',
                      color='darkgreen', bins = 100)

y_ticks = list(range(0, 201, 50))
y_tick_labels_formatted = ['{:,}'.format(ytl) for ytl in y_ticks]   

my_plot.set_yticks(ticks = y_ticks)
my_plot.set_yticklabels(labels = y_tick_labels_formatted, rotation=0)

plt.title(label = "Histogram of missing segment length")
plt.xlabel(xlabel = 'Missing segment length')
plt.ylabel(ylabel = "Count")

#x_ticks = list(range(-3,1))
x_ticks = np.log10(my_start_list)
#x_tick_labels_formatted = ['{:,}'.format(10**xtl) for xtl in x_ticks]   

my_plot.set_xticks(ticks = x_tick_list)
#my_plot.set_xticklabels(labels = x_tick_labels_formatted, rotation=0)

output_file_name = '..\\graphics\\dist_histogram.png'
output_file_name = os.path.normpath(output_file_name)
print(output_file_name)
my_plot.get_figure().savefig(fname = output_file_name)


my_start_list = [100, 250, 500]


for ii in range(1, 5):
    my_calc = int((5280 * (ii / 4)))    
    my_start_list.append(my_calc)    


my_start_list


for ii in range(1, 2):
    for jj in range(0, 5):
        my_calc = int((5280 * (ii / 1)) + (5280 * jj))        
        my_start_list.append(my_calc)    


x_tick_list = [x for x in my_start_list]


x_tick_list



